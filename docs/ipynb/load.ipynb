{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "!pip install autokeras\n",
    "!pip install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "If the data is too large to put in memory all at once, we can load it batch by batch into memory from disk with tf.data.Dataset.\n",
    "This [function](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) can help you build such a tf.data.Dataset for image data.\n",
    "\n",
    "First, we download the data and extract the files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "# local_file_path = tf.keras.utils.get_file(origin=dataset_url, \n",
    "                                          # fname='image_data', \n",
    "                                          # extract=True)\n",
    "# # The file is extracted in the same directory as the downloaded file.\n",
    "# local_dir_path = os.path.dirname(local_file_path)\n",
    "# # After check mannually, we know the extracted data is in 'flower_photos'.\n",
    "# data_dir = os.path.join(local_dir_path, 'flower_photos')\n",
    "# print(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The directory should look like this. Each folder contains the images in the same class.\n",
    "\n",
    "```\n",
    "flowers_photos/\n",
    "  daisy/\n",
    "  dandelion/\n",
    "  roses/\n",
    "  sunflowers/\n",
    "  tulips/\n",
    "```\n",
    "\n",
    "We can split the data into training and testing as we load them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "# train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    # data_dir,\n",
    "    # # Use 20% data as testing data.\n",
    "    # validation_split=0.2,\n",
    "    # subset=\"training\",\n",
    "    # # Set seed to ensure the same split when loading testing data.\n",
    "    # seed=123,\n",
    "    # image_size=(img_height, img_width),\n",
    "    # batch_size=batch_size)\n",
    "\n",
    "# test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    # data_dir,\n",
    "    # validation_split=0.2,\n",
    "    # subset=\"validation\",\n",
    "    # seed=123,\n",
    "    # image_size=(img_height, img_width),\n",
    "    # batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Then we just do one quick demo of AutoKeras to make sure the dataset works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "\n",
    "# clf = ak.ImageClassifier(overwrite=True, max_trials=1)\n",
    "# clf.fit(train_data, epochs=1)\n",
    "# print(clf.evaluate(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "You can also load text datasets in the same way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "local_file_path = tf.keras.utils.get_file(\n",
    "    fname=\"text_data\", \n",
    "    origin=dataset_url, \n",
    "    extract=True,\n",
    ")\n",
    "# The file is extracted in the same directory as the downloaded file.\n",
    "local_dir_path = os.path.dirname(local_file_path)\n",
    "# After check mannually, we know the extracted data is in 'aclImdb'.\n",
    "data_dir = os.path.join(local_dir_path, 'aclImdb')\n",
    "# Remove the unused data folder.\n",
    "import shutil\n",
    "shutil.rmtree(os.path.join(data_dir, 'train/unsup'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "For this dataset, the data is already split into train and test.\n",
    "We just load them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "print(data_dir)\n",
    "train_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'train'),\n",
    "    class_names=['pos', 'neg'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    # shuffle=False,\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'train'),\n",
    "    class_names=['pos', 'neg'],\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    # shuffle=False,\n",
    "    seed=123,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(data_dir, 'test'),\n",
    "    class_names=['pos', 'neg'],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "for x, y in train_data:\n",
    "    print(x.numpy()[0])\n",
    "    print(y.numpy()[0])\n",
    "    # record_x = x.numpy()\n",
    "    # record_y = y.numpy()\n",
    "    break\n",
    "\n",
    "for x, y in train_data:\n",
    "    print(x.numpy()[0])\n",
    "    print(y.numpy()[0])\n",
    "    break\n",
    "\n",
    "# train_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    # os.path.join(data_dir, 'train'),\n",
    "    # class_names=['pos', 'neg'],\n",
    "    # shuffle=True,\n",
    "    # seed=123,\n",
    "    # batch_size=batch_size)\n",
    "\n",
    "# for x, y in train_data:\n",
    "    # for i, a in enumerate(x.numpy()):\n",
    "        # for j, b in enumerate(record_x):\n",
    "            # if a == b:\n",
    "                # print('*')\n",
    "                # assert record_y[j] == y.numpy()[i]\n",
    "\n",
    "# import numpy as np\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# for x, y in train_data:\n",
    "    # for a in x.numpy():\n",
    "        # x_train.append(a)\n",
    "    # for a in y.numpy():\n",
    "        # y_train.append(a)\n",
    "\n",
    "# x_train = np.array(x_train)\n",
    "# y_train = np.array(y_train)\n",
    "\n",
    "# train_data = train_data.shuffle(1000, seed=123, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "clf = ak.TextClassifier(overwrite=True, max_trials=2)\n",
    "# clf.fit(train_data, validation_data=test_data)\n",
    "# clf.fit(train_data, validation_data=train_data)\n",
    "clf.fit(train_data, validation_data=val_data)\n",
    "# clf.fit(x_train, y_train)\n",
    "# clf.fit(train_data)\n",
    "print(clf.evaluate(test_data))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "load",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}